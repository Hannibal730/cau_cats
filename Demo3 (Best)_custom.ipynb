{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d8a97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "img_size = 480  # image: 640 x 480\n",
    "in_channels = 1\n",
    "\n",
    "if in_channels == 1:    \n",
    "    # Data transforms\n",
    "    normalize = transforms.Normalize(mean=[0.5],\n",
    "                                     std=[0.5])\n",
    "else:\n",
    "    # Data transforms\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((int(img_size*1.1), int(img_size*1.1))),  # Resize before cropping\n",
    "    transforms.RandomAffine(degrees=10, translate=(0.05, 0.05), scale=(0.95, 1.05)),\n",
    "    transforms.RandomResizedCrop(img_size, scale=(0.8, 1.0), ratio=(0.9, 1.1)),  # crop focused near center\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.Grayscale(num_output_channels=in_channels),\n",
    "    # transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.05),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.Grayscale(num_output_channels=in_channels),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])\n",
    "\n",
    "import os\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torch\n",
    "\n",
    "# 데이터 경로 설정\n",
    "root_dir = './Refined_mix'\n",
    "base_dataset = ImageFolder(root=root_dir)\n",
    "\n",
    "# 레이블 추출\n",
    "targets = base_dataset.targets\n",
    "\n",
    "# Stratified Split\n",
    "splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx, test_idx = next(splitter.split(range(len(targets)), targets))\n",
    "\n",
    "train_dataset = ImageFolder(root=root_dir, transform=train_transform)\n",
    "test_dataset = ImageFolder(root=root_dir, transform=test_transform)\n",
    "\n",
    "# Subset으로 분할\n",
    "train_dataset = Subset(train_dataset, train_idx)\n",
    "test_dataset = Subset(test_dataset, test_idx)\n",
    "\n",
    "# DataLoader 정의\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36210ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d4802b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fa43e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_len = 32\n",
    "emb_dim = 24\n",
    "n_heads = 4\n",
    "d_layers = 2\n",
    "\n",
    "patch_size = 120\n",
    "num_labels = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2045b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_num = (img_size // patch_size) ** 2\n",
    "seq_len = patch_num*patch_len\n",
    "patch_num, seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e792b2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "# Model parameters\n",
    "model_params = {\n",
    "    'features': 'S',\n",
    "    'seq_len': seq_len,\n",
    "    'pred_len': patch_len*num_labels,\n",
    "    'd_layers': d_layers,\n",
    "    'dec_in': 1, ## channel\n",
    "    'des': 'Exp',\n",
    "    'itr': 1,\n",
    "    'd_model': emb_dim,\n",
    "    'd_ff': emb_dim*2,\n",
    "    'n_heads': n_heads,\n",
    "    'QAM_end': 0.0,\n",
    "    'patch_len': patch_len,\n",
    "    'stride': patch_len, ## patch_len//2\n",
    "    'dropout': 0.1,\n",
    "    'query_independence':False,\n",
    "    'padding_patch':'end',\n",
    "    'store_attn':False,\n",
    "    'QAM_start':0.0,\n",
    "    'classification':True\n",
    "}\n",
    "\n",
    "args = SimpleNamespace(**model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076c5775",
   "metadata": {},
   "outputs": [],
   "source": [
    "from CATS import Model, GEGLU\n",
    "classifier = Model(args=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03a7e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetFront(nn.Module):\n",
    "    def __init__(self, backbone='resnet50', pretrained=True, in_channels=3, out_channels=None):\n",
    "        super().__init__()\n",
    "        resnet = getattr(models, backbone)(pretrained=pretrained)\n",
    "\n",
    "        # conv1 수정 (in_channels가 1인 경우)\n",
    "        if in_channels == 1:\n",
    "            conv1_weight = resnet.conv1.weight\n",
    "            new_conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "            with torch.no_grad():\n",
    "                new_conv1.weight.copy_(conv1_weight.mean(dim=1, keepdim=True))\n",
    "            resnet.conv1 = new_conv1\n",
    "        elif in_channels != 3:\n",
    "            raise ValueError(\"Only in_channels=1 or 3 supported for now.\")\n",
    "\n",
    "        # 앞단 구성\n",
    "        self.front = nn.Sequential(\n",
    "            resnet.conv1,\n",
    "            resnet.bn1,\n",
    "            resnet.relu,\n",
    "            resnet.maxpool,\n",
    "            resnet.layer1,\n",
    "        )\n",
    "\n",
    "        # 원래 layer1까지의 출력 채널 수\n",
    "        self.base_out_channels = self._get_layer1_out_channels(backbone)\n",
    "\n",
    "        # 채널 변경이 필요한 경우 1x1 conv 추가\n",
    "        if out_channels is not None and out_channels != self.base_out_channels:\n",
    "            self.channel_proj = nn.Conv2d(self.base_out_channels, out_channels, kernel_size=1)\n",
    "        else:\n",
    "            self.channel_proj = nn.Identity()\n",
    "\n",
    "    def _get_layer1_out_channels(self, backbone):\n",
    "        \"\"\"ResNet 구조에 따라 layer1의 출력 채널 수 리턴\"\"\"\n",
    "        if '18' in backbone or '34' in backbone:\n",
    "            return 64\n",
    "        elif '50' in backbone or '101' in backbone or '152' in backbone:\n",
    "            return 256\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown backbone: {backbone}\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.front(x)\n",
    "        x = self.channel_proj(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02a0712",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PatchConvEncoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_channels=3,\n",
    "                 img_size=224,\n",
    "                 patch_size=16, \n",
    "                 hidden_dim=128, \n",
    "                 output_dim=2048):\n",
    "        super(PatchConvEncoder, self).__init__()\n",
    "        \n",
    "        self.patch_size = patch_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # 224x224 이미지를 patch_size x patch_size 로 나눌 경우\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        \n",
    "        # Shared conv module for all patches\n",
    "        self.shared_conv = nn.Sequential(\n",
    "            # nn.Conv2d(in_channels, hidden_dim, kernel_size=3, padding=1),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Conv2d(hidden_dim, hidden_dim, kernel_size=3, padding=1),\n",
    "            # nn.ReLU(),\n",
    "            # nn.MaxPool2d(2),  # 각 patch를 1x1로 압축\n",
    "            # nn.Conv2d(hidden_dim, hidden_dim, kernel_size=3, padding=1),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Conv2d(hidden_dim, hidden_dim, kernel_size=3, padding=1),\n",
    "            # nn.ReLU(),\n",
    "            ResNetFront(backbone='resnet50', pretrained=True, in_channels=in_channels, out_channels=hidden_dim),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))  # 각 patch를 1x1로 압축\n",
    "        )\n",
    "\n",
    "        if output_dim is None:\n",
    "            self.proj = None\n",
    "        else:\n",
    "            # 최종 projection layer (concat 후 → output_dim으로)\n",
    "            self.proj = nn.Linear(self.num_patches * hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape  # B x 3 x 224 x 224\n",
    "\n",
    "        # 패치를 나눔\n",
    "        patches = x.unfold(2, self.patch_size, self.patch_size)\\\n",
    "                   .unfold(3, self.patch_size, self.patch_size)  # B x C x N_patch_h x N_patch_w x patch_size x patch_size\n",
    "\n",
    "        N_h, N_w = patches.shape[2], patches.shape[3]\n",
    "        patches = patches.permute(0, 2, 3, 1, 4, 5)  # B x N_patch_h x N_patch_w x C x patch_size x patch_size\n",
    "        patches = patches.reshape(-1, C, self.patch_size, self.patch_size)  # (B * N_patches) x C x patch_size x patch_size\n",
    "\n",
    "        # shared conv 적용\n",
    "        conv_outs = self.shared_conv(patches)  # (B * N_patches) x hidden_dim x 1 x 1\n",
    "        conv_outs = conv_outs.view(conv_outs.size(0), -1)  # (B * N_patches) x hidden_dim\n",
    "\n",
    "        # 원래 배치 단위로 reshape\n",
    "        conv_outs = conv_outs.view(B, self.num_patches * self.hidden_dim)  # B x (num_patches * hidden_dim)\n",
    "\n",
    "        # projection\n",
    "        if self.proj is None:\n",
    "            out = conv_outs\n",
    "        else:\n",
    "            out = self.proj(conv_outs)  # B x output_dim\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da95246",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = PatchConvEncoder(in_channels=in_channels, img_size=img_size, patch_size=patch_size, \n",
    "                           hidden_dim=patch_len, output_dim=None)\n",
    "x = torch.randn(8, in_channels, img_size, img_size)  # batch size = 8\n",
    "y = encoder(x)\n",
    "print(y.shape)  # torch.Size([8, 2048])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cb4022",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XModel(torch.nn.Module):\n",
    "    def __init__(self, encoder, classifier):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.classifier = classifier\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x).unsqueeze(-1)\n",
    "        out = self.classifier(x).squeeze()\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92493545",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XModel(encoder, classifier).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a24704",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_trainable_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "count_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cace39a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(8, in_channels, img_size, img_size)  # batch size = 8\n",
    "y = model(x.to(device))\n",
    "print(y.shape)  # torch.Size([8, 2048])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eddb699",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "fc72a1ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[88], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     28\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     31\u001b[0m     images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:734\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    731\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    733\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 734\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    736\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    739\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    740\u001b[0m ):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:790\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    789\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 790\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    791\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    792\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:50\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataset.py:416\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataset.py:416\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/datasets/folder.py:247\u001b[0m, in \u001b[0;36mDatasetFolder.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    245\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader(path)\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 247\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    249\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/transforms/transforms.py:713\u001b[0m, in \u001b[0;36mRandomHorizontalFlip.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    706\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    707\u001b[0m \u001b[38;5;124;03m    img (PIL Image or Tensor): Image to be flipped.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;124;03m    PIL Image or Tensor: Randomly flipped image.\u001b[39;00m\n\u001b[1;32m    711\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    712\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mp:\n\u001b[0;32m--> 713\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhflip\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    714\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/transforms/functional.py:666\u001b[0m, in \u001b[0;36mhflip\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mhflip\u001b[39m(img: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    655\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Horizontally flip the given image.\u001b[39;00m\n\u001b[1;32m    656\u001b[0m \n\u001b[1;32m    657\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    664\u001b[0m \u001b[38;5;124;03m        PIL Image or Tensor:  Horizontally flipped image.\u001b[39;00m\n\u001b[1;32m    665\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 666\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_tracing\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    667\u001b[0m         _log_api_usage_once(hflip)\n\u001b[1;32m    668\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mTensor):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/jit/_trace.py:1327\u001b[0m, in \u001b[0;36mis_tracing\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1322\u001b[0m         torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39m_trace\u001b[38;5;241m.\u001b[39m_trace_module_map \u001b[38;5;241m=\u001b[39m old_module_map\n\u001b[1;32m   1324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m module\n\u001b[0;32m-> 1327\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mis_tracing\u001b[39m():\n\u001b[1;32m   1328\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a boolean value.\u001b[39;00m\n\u001b[1;32m   1329\u001b[0m \n\u001b[1;32m   1330\u001b[0m \u001b[38;5;124;03m    Returns ``True`` in tracing (if a function is called during the\u001b[39;00m\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;124;03m    tracing of code with ``torch.jit.trace``) and ``False`` otherwise.\u001b[39;00m\n\u001b[1;32m   1332\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1333\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_scripting():\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "\n",
    "def count_trainable_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"#Params: \", count_trainable_parameters(model))\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "import schedulefree\n",
    "optimizer = schedulefree.AdamWScheduleFree(model.parameters(), lr=0.001)\n",
    "# optimizer = schedulefree.SGDScheduleFree(model.parameters(), lr=0.01, weight_decay=5e-3, momentum=0.9)\n",
    "schdeulefree_flag = True\n",
    "\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=5e-3, momentum=0.9)\n",
    "# optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "# schdeulefree_flag = False\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    if schdeulefree_flag:\n",
    "        optimizer.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Metrics\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}, Accuracy: {100 * correct / total:.2f}%')\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    model.eval()  # 평가 모드\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            # 예측 결과 및 실제 라벨 저장\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # 정확도\n",
    "    accuracy = 100 * correct / total\n",
    "    \n",
    "    # 정밀도, 재현율, F1 점수 계산 (다중 클래스 기준, macro 평균)\n",
    "    precision = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    \n",
    "    # 출력\n",
    "    print(f'Test Accuracy : {accuracy:.2f}%')\n",
    "    print(f'Precision      : {precision:.4f}')\n",
    "    print(f'Recall         : {recall:.4f}')\n",
    "    print(f'F1 Score       : {f1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d15eb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# --- 1. 모델과 더미 입력 데이터 준비 ---\n",
    "# 이미 생성된 모델을 eval 모드로 설정\n",
    "model.eval()\n",
    "\n",
    "# 추론할 단일 이미지에 해당하는 더미 텐서 생성\n",
    "# 형태: [배치 크기=1, 채널 수, 이미지 높이, 이미지 너비]\n",
    "dummy_input = torch.randn(1, in_channels, img_size, img_size).to(device)\n",
    "\n",
    "# --- 2. 메모리 측정 ---\n",
    "# 측정을 시작하기 전 GPU 캐시를 비우고 통계를 리셋합니다.\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats(device)\n",
    "\n",
    "# torch.no_grad() 컨텍스트 안에서 추론을 실행하여 그래디언트 계산을 방지합니다.\n",
    "with torch.no_grad():\n",
    "    # 모델에 더미 입력을 통과시켜 순전파를 실행합니다.\n",
    "    output = model(dummy_input)\n",
    "\n",
    "# 순전파 중 할당되었던 최대 메모리를 가져옵니다. (단위: 바이트)\n",
    "peak_memory_bytes = torch.cuda.max_memory_allocated(device)\n",
    "\n",
    "# 보기 쉽게 메가바이트(MB) 단위로 변환합니다.\n",
    "peak_memory_mb = peak_memory_bytes / (1024 * 1024)\n",
    "\n",
    "print(f\"추론 시 최대 GPU 메모리 사용량: {peak_memory_mb:.2f} MB\")\n",
    "\n",
    "# (참고) 현재 시점에 할당된 메모리 (모델 가중치 + 출력 텐서 등)\n",
    "current_memory_mb = torch.cuda.memory_allocated(device) / (1024 * 1024)\n",
    "print(f\"현재 할당된 GPU 메모리: {current_memory_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd2ce97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ext",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
