{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab12657",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from types import SimpleNamespace\n",
    "import numpy as np\n",
    "import yaml\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# (주의) CATS.py 파일이 같은 디렉터리에 있어야 합니다.\n",
    "# 파일이 없다면 'ModuleNotFoundError'가 발생합니다.\n",
    "try:\n",
    "    from CATS import Model as CATS_Model\n",
    "except ModuleNotFoundError:\n",
    "    print(\"=\"*50)\n",
    "    print(\"오류: 'CATS.py' 파일을 찾을 수 없습니다.\")\n",
    "    print(\"run.ipynb와 같은 디렉터리에 CATS.py를 위치시켜주세요.\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "# (선택) tqdm은 진행률 표시줄을 위해 사용됩니다.\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3a9d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_logging(data_dir):\n",
    "    \"\"\"로그 파일을 log 폴더에 생성하고, 콘솔(노트북 출력)에도 함께 출력하도록 설정합니다.\"\"\"\n",
    "    data_dir_name = os.path.basename(os.path.normpath(data_dir))\n",
    "    log_dir = os.path.join(\"log\", data_dir_name)\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    log_filename = os.path.join(log_dir, f\"log_{timestamp}.log\")\n",
    "    \n",
    "    # (중요) 주피터 노트북 환경에서는 핸들러를 재설정해야 할 수 있습니다.\n",
    "    # 이전에 로거가 설정되었다면, 기존 핸들러를 제거합니다.\n",
    "    logger = logging.getLogger()\n",
    "    if logger.hasHandlers():\n",
    "        logger.handlers.clear()\n",
    "        \n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_filename),\n",
    "            logging.StreamHandler() # 콘솔(노트북 출력)\n",
    "        ]\n",
    "    )\n",
    "    logging.info(f\"Log 기록 시작. 로그 파일: '{log_filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25362dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CnnFeatureExtractor(nn.Module):\n",
    "    \"\"\"\n",
    "    다양한 CNN 아키텍처의 앞부분을 특징 추출기로 사용하는 범용 클래스입니다.\n",
    "    run.yaml의 `cnn_feature_extractor.name` 설정에 따라 모델 구조가 결정됩니다.\n",
    "    \"\"\"\n",
    "    def __init__(self, cnn_feature_extractor_name='resnet18_layer1', pretrained=True, in_channels=3, out_channels=None):\n",
    "        super().__init__()\n",
    "        self.cnn_feature_extractor_name = cnn_feature_extractor_name\n",
    "        \n",
    "        # CNN 모델 이름에 따라 모델과 잘라낼 레이어, 기본 출력 채널을 설정합니다.\n",
    "        if cnn_feature_extractor_name == 'resnet18_layer1':\n",
    "            base_model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1 if pretrained else None)\n",
    "            self._adjust_input_channels(base_model, in_channels)\n",
    "            self.front = nn.Sequential(*list(base_model.children())[:5]) # layer1까지\n",
    "            base_out_channels = 64\n",
    "        elif cnn_feature_extractor_name == 'resnet18_layer2':\n",
    "            base_model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1 if pretrained else None)\n",
    "            self._adjust_input_channels(base_model, in_channels)\n",
    "            self.front = nn.Sequential(*list(base_model.children())[:6]) # layer2까지\n",
    "            base_out_channels = 128\n",
    "        elif cnn_feature_extractor_name == 'mobilenet_v3_small_feat1':\n",
    "            base_model = models.mobilenet_v3_small(weights=models.MobileNet_V3_Small_Weights.IMAGENET1K_V1 if pretrained else None)\n",
    "            self._adjust_input_channels(base_model, in_channels)\n",
    "            self.front = base_model.features[:2] # features의 2번째 블록까지\n",
    "            base_out_channels = 16\n",
    "        elif cnn_feature_extractor_name == 'mobilenet_v3_small_feat3':\n",
    "            base_model = models.mobilenet_v3_small(weights=models.MobileNet_V3_Small_Weights.IMAGENET1K_V1 if pretrained else None)\n",
    "            self._adjust_input_channels(base_model, in_channels)\n",
    "            self.front = base_model.features[:4] # features의 4번째 블록까지\n",
    "            base_out_channels = 24\n",
    "        elif cnn_feature_extractor_name == 'efficientnet_b0_feat2':\n",
    "            base_model = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.IMAGENET1K_V1 if pretrained else None)\n",
    "            self._adjust_input_channels(base_model, in_channels)\n",
    "            self.front = base_model.features[:3] # features의 3번째 블록까지\n",
    "            base_out_channels = 24\n",
    "        elif cnn_feature_extractor_name == 'efficientnet_b0_feat3':\n",
    "            base_model = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.IMAGENET1K_V1 if pretrained else None)\n",
    "            self._adjust_input_channels(base_model, in_channels)\n",
    "            self.front = base_model.features[:4] # features의 4번째 블록까지\n",
    "            base_out_channels = 40\n",
    "        else:\n",
    "            raise ValueError(f\"지원하지 않는 CNN 피처 추출기 이름입니다: {cnn_feature_extractor_name}\")\n",
    "\n",
    "        # 최종 출력 채널 수를 `featured_patch_channel`에 맞추기 위한 1x1 컨볼루션 레이어입니다.\n",
    "        if out_channels is not None and out_channels != base_out_channels:\n",
    "            logging.info(f\"CnnFeatureExtractor: 출력 채널을 {base_out_channels}에서 {out_channels}로 프로젝션합니다.\")\n",
    "            self.channel_proj = nn.Conv2d(base_out_channels, out_channels, kernel_size=1)\n",
    "        else:\n",
    "            self.channel_proj = nn.Identity()\n",
    "\n",
    "    def _adjust_input_channels(self, base_model, in_channels):\n",
    "        \"\"\"모델의 첫 번째 컨볼루션 레이어의 입력 채널을 조정합니다.\"\"\"\n",
    "        if in_channels == 1:\n",
    "            # 첫 번째 conv 레이어 찾기\n",
    "            if 'resnet' in self.cnn_feature_extractor_name:\n",
    "                first_conv = base_model.conv1\n",
    "                logging.info(f\"ResNet 첫 번째 Conv (conv1) 채널을 {first_conv.in_channels}에서 {in_channels}로 변경합니다.\")\n",
    "                out_c, _, k, s, p, _, _, _ = first_conv.out_channels, first_conv.in_channels, first_conv.kernel_size, first_conv.stride, first_conv.padding, first_conv.dilation, first_conv.groups, first_conv.bias\n",
    "                new_conv = nn.Conv2d(1, out_c, kernel_size=k, stride=s, padding=p, bias=False)\n",
    "                with torch.no_grad():\n",
    "                    new_conv.weight.copy_(first_conv.weight.mean(dim=1, keepdim=True))\n",
    "                base_model.conv1 = new_conv\n",
    "            elif 'mobilenet' in self.cnn_feature_extractor_name or 'efficientnet' in self.cnn_feature_extractor_name:\n",
    "                first_conv = base_model.features[0][0] # nn.Sequential -> Conv2dNormActivation -> Conv2d\n",
    "                logging.info(f\"MobileNet/EfficientNet 첫 번째 Conv 채널을 {first_conv.in_channels}에서 {in_channels}로 변경합니다.\")\n",
    "                out_c, _, k, s, p, _, _, _ = first_conv.out_channels, first_conv.in_channels, first_conv.kernel_size, first_conv.stride, first_conv.padding, first_conv.dilation, first_conv.groups, first_conv.bias\n",
    "                new_conv = nn.Conv2d(1, out_c, kernel_size=k, stride=s, padding=p, bias=False)\n",
    "                with torch.no_grad():\n",
    "                    new_conv.weight.copy_(first_conv.weight.mean(dim=1, keepdim=True))\n",
    "                base_model.features[0][0] = new_conv\n",
    "        elif in_channels != 3:\n",
    "            raise ValueError(\"in_channels는 1 또는 3만 지원합니다.\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.front(x)\n",
    "        x = self.channel_proj(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d65fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchConvEncoder(nn.Module):\n",
    "    \"\"\"이미지를 패치로 나누고, 각 패치에서 특징을 추출하여 1D 시퀀스로 변환하는 인코더입니다.\"\"\"\n",
    "    def __init__(self, in_channels, img_size, patch_size, hidden_dim, cnn_feature_extractor_name, output_dim=None):\n",
    "        super(PatchConvEncoder, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        \n",
    "        self.shared_conv = nn.Sequential(\n",
    "            CnnFeatureExtractor(cnn_feature_extractor_name=cnn_feature_extractor_name, pretrained=True, in_channels=in_channels, out_channels=hidden_dim),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "\n",
    "        if output_dim is not None:\n",
    "            self.proj = nn.Linear(self.num_patches * hidden_dim, output_dim)\n",
    "        else:\n",
    "            self.proj = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        # 1. 이미지를 패치로 분할\n",
    "        patches = x.unfold(2, self.patch_size, self.patch_size).unfold(3, self.patch_size, self.patch_size)\n",
    "        # (B, C, H_patch_num, W_patch_num, patch_size, patch_size) -> (B, H_num, W_num, C, H_patch, W_patch)\n",
    "        patches = patches.permute(0, 2, 3, 1, 4, 5).reshape(-1, C, self.patch_size, self.patch_size)\n",
    "        \n",
    "        # 2. 공유 CNN 인코더로 특징 추출 (Batch * num_patches, C, H_patch, W_patch) -> (Batch * num_patches, hidden_dim, 1, 1)\n",
    "        conv_outs = self.shared_conv(patches)\n",
    "        # (Batch * num_patches, hidden_dim)\n",
    "        conv_outs = conv_outs.view(conv_outs.size(0), -1)\n",
    "        \n",
    "        # 3. 1D 시퀀스로 펼치기 (B, num_patches * hidden_dim)\n",
    "        conv_outs = conv_outs.view(B, self.num_patches * self.hidden_dim)\n",
    "        \n",
    "        if self.proj is not None:\n",
    "            conv_outs = self.proj(conv_outs)\n",
    "            \n",
    "        return conv_outs\n",
    "\n",
    "class HybridModel(torch.nn.Module):\n",
    "    \"\"\"인코더와 CATS 분류기를 결합한 최종 하이브리드 모델입니다.\"\"\"\n",
    "    def __init__(self, encoder, classifier):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.classifier = classifier\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 1. 인코딩: 2D 이미지 -> 1D 시퀀스 (B, SeqLen)\n",
    "        x = self.encoder(x)\n",
    "        # 2. CATS 입력 형식 맞추기: (B, SeqLen) -> (B, SeqLen, 1)\n",
    "        x = x.unsqueeze(-1)\n",
    "        # 3. 분류: (B, SeqLen, 1) -> (B, NumClasses)\n",
    "        out = self.classifier(x).squeeze(-1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05ec2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_model_parameters(model):\n",
    "    \"\"\"모델의 구간별 및 총 파라미터 수를 계산하고 로깅합니다.\"\"\"\n",
    "    \n",
    "    def count_parameters(m):\n",
    "        return sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
    "\n",
    "    encoder_params = count_parameters(model.encoder)\n",
    "    classifier_params = count_parameters(model.classifier)\n",
    "    total_params = encoder_params + classifier_params\n",
    "\n",
    "    logging.info(\"=\"*50)\n",
    "    logging.info(\"모델 파라미터 수:\")\n",
    "    logging.info(f\"  - Encoder (PatchConvEncoder): {encoder_params:,} 개\")\n",
    "    logging.info(f\"  - Classifier (CATS_Model):    {classifier_params:,} 개\")\n",
    "    logging.info(f\"  - 총 파라미터:                  {total_params:,} 개\")\n",
    "    logging.info(\"=\"*50)\n",
    "\n",
    "def evaluate(model, test_loader, device):\n",
    "    \"\"\"모델을 평가하고 정확도, 정밀도, 재현율, F1 점수를 로깅합니다.\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(test_loader, desc=\"Evaluating\", leave=False):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    if total == 0:\n",
    "        logging.warning(\"테스트 데이터가 없습니다. 평가를 건너뜁니다.\")\n",
    "        return 0.0\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    precision = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    \n",
    "    logging.info(f'Test Accuracy: {accuracy:.2f}% | Precision: {precision:.4f} | Recall: {recall:.4f} | F1 Score: {f1:.4f}')\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86cd474",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model, train_loader, valid_loader, device):\n",
    "    \"\"\"모델 훈련 및 평가를 수행하고 최고 성능 모델을 저장합니다.\"\"\"\n",
    "    logging.info(\"훈련 모드를 시작합니다.\")\n",
    "    \n",
    "    data_dir_name = os.path.basename(os.path.normpath(args.data_dir))\n",
    "    checkpoints_dir = os.path.join(\"checkpoints\", data_dir_name)\n",
    "    os.makedirs(checkpoints_dir, exist_ok=True)\n",
    "    model_path = os.path.join(checkpoints_dir, args.model_path)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=args.lr)\n",
    "    \n",
    "    best_f1 = 0.0\n",
    "\n",
    "    for epoch in range(args.epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        # tqdm을 사용하여 진행률 표시\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{args.epochs} [Train]\", leave=False)\n",
    "        for images, labels in pbar:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            pbar.set_postfix(loss=f\"{running_loss/len(pbar):.4f}\")\n",
    "\n",
    "        train_acc = 100 * correct / total\n",
    "        logging.info(f'Epoch [{epoch+1}/{args.epochs}], Loss: {running_loss/len(train_loader):.4f}, Train Accuracy: {train_acc:.2f}%')\n",
    "        \n",
    "        # --- 평가 단계 ---\n",
    "        f1 = evaluate(model, valid_loader, device)\n",
    "        \n",
    "        # 최고 성능 모델 저장\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            logging.info(f\"최고 성능 모델 저장 완료 (F1 Score: {best_f1:.4f}) -> '{model_path}'\")\n",
    "\n",
    "def inference(args, model, data_loader, device, mode_name=\"추론\"):\n",
    "    \"\"\"저장된 모델을 불러와 추론 시 GPU 메모리 사용량을 측정하고, 테스트셋 성능을 평가합니다.\"\"\"\n",
    "    logging.info(f\"{mode_name} 모드를 시작합니다.\")\n",
    "    \n",
    "    data_dir_name = os.path.basename(os.path.normpath(args.data_dir))\n",
    "    # (수정) 원본 스크립트의 'Sewer-ML' 하드코딩 대신 data_dir_name 사용\n",
    "    model_path = os.path.join(\"checkpoints\", data_dir_name, args.model_path)\n",
    "    if not os.path.exists(model_path):\n",
    "        logging.error(f\"모델 파일('{model_path}')을 찾을 수 없습니다. 먼저 훈련을 실행하세요.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "        logging.info(f\"'{model_path}' 가중치 로드 완료.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"모델 가중치 로딩 중 오류 발생: {e}\")\n",
    "        return\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    # 1. GPU 메모리 사용량 측정\n",
    "    dummy_input = torch.randn(1, args.in_channels, args.img_size, args.img_size).to(device)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.reset_peak_memory_stats(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(dummy_input)\n",
    "        \n",
    "    if torch.cuda.is_available():\n",
    "        peak_memory_bytes = torch.cuda.max_memory_allocated(device)\n",
    "        peak_memory_mb = peak_memory_bytes / (1024 * 1024)\n",
    "        logging.info(f\"추론 시 최대 GPU 메모리 사용량: {peak_memory_mb:.2f} MB\")\n",
    "    else:\n",
    "        logging.info(\"CUDA를 사용할 수 없어 GPU 메모리 사용량을 측정할 수 없습니다.\")\n",
    "\n",
    "    # 2. 테스트셋 성능 평가\n",
    "    evaluate(model, data_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf20d0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(run_cfg, train_cfg, model_cfg, data_dir_name):\n",
    "    \"\"\"데이터셋을 로드하고 전처리하여 DataLoader를 생성합니다.\"\"\"\n",
    "    normalize = transforms.Normalize(mean=[0.5]*model_cfg.in_channels, std=[0.5]*model_cfg.in_channels)\n",
    "    \n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((int(model_cfg.img_size*1.1), int(model_cfg.img_size*1.1))),\n",
    "        transforms.RandomAffine(degrees=10, translate=(0.05, 0.05), scale=(0.95, 1.05)),\n",
    "        transforms.RandomResizedCrop(model_cfg.img_size, scale=(0.8, 1.0), ratio=(0.9, 1.1)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.Grayscale(num_output_channels=model_cfg.in_channels),\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "\n",
    "    valid_test_transform = transforms.Compose([\n",
    "        transforms.Resize((model_cfg.img_size, model_cfg.img_size)),\n",
    "        transforms.Grayscale(num_output_channels=model_cfg.in_channels),\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "    \n",
    "    # 데이터 분할 로직 대신, 각 폴더를 직접 로드\n",
    "    try:\n",
    "        logging.info(\"데이터 로드를 시작합니다.\")\n",
    "        logging.info(f\"Train 경로: {run_cfg.train_dir}\")\n",
    "        logging.info(f\"Valid 경로: {run_cfg.valid_dir}\")\n",
    "        logging.info(f\"Test 경로:  {run_cfg.test_dir}\")\n",
    "\n",
    "        # 전체 데이터셋 로드\n",
    "        full_train_dataset = datasets.ImageFolder(root=run_cfg.train_dir, transform=train_transform)\n",
    "        full_valid_dataset = datasets.ImageFolder(root=run_cfg.valid_dir, transform=valid_test_transform)\n",
    "        full_test_dataset = datasets.ImageFolder(root=run_cfg.test_dir, transform=valid_test_transform)\n",
    "\n",
    "        num_labels = len(full_train_dataset.classes)\n",
    "        # 클래스 이름이 모든 데이터셋에서 동일한지 확인\n",
    "        assert full_train_dataset.classes == full_valid_dataset.classes == full_test_dataset.classes, \"Train/Valid/Test의 클래스가 일치하지 않습니다.\"\n",
    "\n",
    "        # --- 데이터 샘플링 로직 ---\n",
    "        sampling_ratio = getattr(run_cfg, 'sampling_ratio', 1.0)\n",
    "        if sampling_ratio < 1.0:\n",
    "            logging.info(f\"데이터셋을 {sampling_ratio * 100:.0f}% 비율로 샘플링합니다 (random_state={run_cfg.random_state}).\")\n",
    "            \n",
    "            def get_subset(dataset):\n",
    "                targets = [s[1] for s in dataset.samples]\n",
    "                splitter = StratifiedShuffleSplit(n_splits=1, train_size=sampling_ratio, random_state=run_cfg.random_state)\n",
    "                subset_indices, _ = next(splitter.split(np.zeros(len(targets)), targets))\n",
    "                return Subset(dataset, subset_indices)\n",
    "\n",
    "            train_dataset = get_subset(full_train_dataset)\n",
    "            valid_dataset = get_subset(full_valid_dataset)\n",
    "            test_dataset = get_subset(full_test_dataset)\n",
    "        else:\n",
    "            logging.info(\"전체 데이터셋을 사용합니다 (sampling_ratio=1.0).\")\n",
    "            train_dataset = full_train_dataset\n",
    "            valid_dataset = full_valid_dataset\n",
    "            test_dataset = full_test_dataset\n",
    "\n",
    "        # DataLoader 생성\n",
    "        train_loader = DataLoader(train_dataset, batch_size=train_cfg.batch_size, shuffle=True, num_workers=os.cpu_count()//2)\n",
    "        valid_loader = DataLoader(valid_dataset, batch_size=train_cfg.batch_size, shuffle=False, num_workers=os.cpu_count()//2)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=train_cfg.batch_size, shuffle=False, num_workers=os.cpu_count()//2)\n",
    "        \n",
    "        logging.info(f\"훈련 데이터: {len(train_dataset)}개, 검증 데이터: {len(valid_dataset)}개, 테스트 데이터: {len(test_dataset)}개\")\n",
    "        \n",
    "        return train_loader, valid_loader, test_loader, num_labels, full_train_dataset.classes\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        logging.error(f\"데이터 폴더를 찾을 수 없습니다. 'run.yaml'의 'train_dir', 'valid_dir', 'test_dir' 경로를 확인해주세요.\")\n",
    "        logging.error(f\"오류 상세: {e}\")\n",
    "        # 노트북에서는 exit() 대신 None을 반환\n",
    "        return None, None, None, None, None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"데이터 준비 중 오류 발생: {e}\")\n",
    "        return None, None, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622bd47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 설정 로드 ---\n",
    "# (노트북 환경) argparse 대신 설정 파일 경로를 직접 지정\n",
    "config_path = 'run.yaml'\n",
    "\n",
    "with open(config_path, 'r', encoding='utf-8') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# SimpleNamespace를 사용하여 딕셔너리처럼 접근 가능하게 변환\n",
    "run_cfg = SimpleNamespace(**config['run'])\n",
    "train_cfg = SimpleNamespace(**config['training'])\n",
    "model_cfg = SimpleNamespace(**config['model'])\n",
    "cats_cfg = SimpleNamespace(**config['model']['cats'])\n",
    "\n",
    "# (노트북 환경) 데이터셋 이름을 기반으로 로깅 설정\n",
    "# train_dir에서 'Sewer-ML' 같은 기본 이름을 추출\n",
    "try:\n",
    "    # 'data/Sewer-ML/train' -> 'data/Sewer-ML' -> 'Sewer-ML'\n",
    "    data_dir_basename = os.path.basename(os.path.normpath(os.path.dirname(run_cfg.train_dir)))\n",
    "    if not data_dir_basename: # 만약 'train' 폴더가 루트에 있다면\n",
    "        data_dir_basename = \"default_run\"\n",
    "except Exception as e:\n",
    "    data_dir_basename = \"default_run\"\n",
    "    print(f\"데이터 경로에서 기본 이름 추출 실패: {e}. 'default_run' 사용.\")\n",
    "    \n",
    "setup_logging(data_dir_basename)\n",
    "\n",
    "# --- 설정 파일 내용 로깅 ---\n",
    "config_str = yaml.dump(config, allow_unicode=True, default_flow_style=False, sort_keys=False)\n",
    "logging.info(\"=\"*50)\n",
    "logging.info(f\"'{config_path}' 로드 완료:\")\n",
    "logging.info(\"\\n\" + config_str)\n",
    "logging.info(\"=\"*50)\n",
    "\n",
    "# --- 공통 파라미터 설정 ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logging.info(f\"사용 디바이스: {device}\")\n",
    "\n",
    "# --- 데이터 준비 ---\n",
    "train_loader, valid_loader, test_loader, num_labels, class_names = prepare_data(run_cfg, train_cfg, model_cfg, data_dir_basename)\n",
    "\n",
    "# 데이터 로드 실패 시 중단\n",
    "if train_loader is None:\n",
    "    logging.error(\"데이터 로드에 실패하여 실행을 중단합니다.\")\n",
    "else:\n",
    "    logging.info(f\"클래스 ({num_labels}개): {class_names}\")\n",
    "    \n",
    "    # --- 모델 구성 ---\n",
    "    patch_num = (model_cfg.img_size // model_cfg.patch_size) ** 2\n",
    "    seq_len = patch_num * cats_cfg.featured_patch_channel\n",
    "    logging.info(f\"이미지 크기: {model_cfg.img_size}, 패치 크기: {model_cfg.patch_size} -> 패치 수: {patch_num}\")\n",
    "    logging.info(f\"CATS 입력 시퀀스 길이 (SeqLen = patch_num * featured_patch_channel): {patch_num} * {cats_cfg.featured_patch_channel} = {seq_len}\")\n",
    "    \n",
    "    cats_params = {\n",
    "        'seq_len': seq_len, 'pred_len': num_labels, 'd_layers': cats_cfg.d_layers,\n",
    "        'dec_in': model_cfg.in_channels, # CATS 원본의 파라미터 (분류 시 사용 안 됨)\n",
    "        'd_model': cats_cfg.emb_dim,\n",
    "        'd_ff': cats_cfg.emb_dim * cats_cfg.d_ff_ratio,\n",
    "        'n_heads': cats_cfg.n_heads,\n",
    "        'patch_len': cats_cfg.featured_patch_channel,\n",
    "        'stride': cats_cfg.featured_patch_channel,\n",
    "        'classification': cats_cfg.classification,\n",
    "        'dropout': cats_cfg.dropout,\n",
    "        'channel_independence': cats_cfg.channel_independence,\n",
    "        'padding_patch': cats_cfg.padding_patch,\n",
    "        'store_attn': cats_cfg.store_attn,\n",
    "        'QAM_start': cats_cfg.qam['start'],\n",
    "        'QAM_end': cats_cfg.qam['end'],\n",
    "    }\n",
    "    cats_args = SimpleNamespace(**cats_params)\n",
    "\n",
    "    # cli_args 대신 설정 파일 값들을 전달\n",
    "    cli_args = SimpleNamespace(\n",
    "        mode=run_cfg.mode, \n",
    "        data_dir=data_dir_basename, # 데이터셋 대표 이름\n",
    "        batch_size=train_cfg.batch_size,\n",
    "        epochs=train_cfg.epochs, \n",
    "        lr=train_cfg.lr, \n",
    "        model_path=run_cfg.model_path,\n",
    "        img_size=model_cfg.img_size, \n",
    "        in_channels=model_cfg.in_channels\n",
    "    )\n",
    "\n",
    "    encoder = PatchConvEncoder(in_channels=model_cfg.in_channels, img_size=model_cfg.img_size, patch_size=model_cfg.patch_size, \n",
    "                               hidden_dim=cats_cfg.featured_patch_channel, cnn_feature_extractor_name=model_cfg.cnn_feature_extractor['name'])\n",
    "    classifier = CATS_Model(args=cats_args)\n",
    "    model = HybridModel(encoder, classifier).to(device)\n",
    "\n",
    "    # 모델 생성 후 파라미터 수 로깅\n",
    "    log_model_parameters(model)\n",
    "    \n",
    "    # --- 모드에 따라 실행 ---\n",
    "    if run_cfg.mode == 'train':\n",
    "        # 훈련 시에는 train_loader와 valid_loader 사용\n",
    "        train(cli_args, model, train_loader, valid_loader, device)\n",
    "        \n",
    "        logging.info(\"=\"*50)\n",
    "        logging.info(\"훈련 완료. 최종 모델 성능을 테스트 세트로 평가합니다.\")\n",
    "        # 훈련 종료 후, 최종 평가를 위해 test_loader 사용.\n",
    "        inference(cli_args, model, test_loader, device, mode_name=\"Final Evaluation\")\n",
    "    elif run_cfg.mode == 'inference':\n",
    "        # 추론 모드에서는 test_loader를 사용해 성능 평가\n",
    "        inference(cli_args, model, test_loader, device, mode_name=\"Inference\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
